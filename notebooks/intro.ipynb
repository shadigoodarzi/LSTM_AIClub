{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stock Market Predictions with a Long Short-Term Memory Neural Network\n",
    "\n",
    "Say you're planning to invest in the stock market, so you want to model fluctuations in price by looking at the history of a sequence of prices to accurately predict what future prices will be. When analyzing a sequence of data which were observed in some constant increment of time, and each observation is directly dependent on one or more previous observations (a stock price tomorrow directly depends on its price today), you need a time series model. In this workshop, we'll start by investigating two well-known models, then compare their prediction accuracy to an LSTM nueral network.\n",
    "\n",
    "Adapted from: https://www.datacamp.com/community/tutorials/lstm-python-stock-market\n",
    "\n",
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The tutorial that this workshop was adapted from outlines two sources of data for use in the remainder of the workshop. For simplicity, we'll stick with the Kaggle data set that was provided. Feel free to tinker with analyzing different stock symbols (i.e., data for different companies' stock prices), although I can't guarantee that everything will work as I've only tested the code with the Kaggle data for Hewlett-Packard (HP). Generally speaking, stock prices can be measured with the following metrics:\n",
    "- Open: Opening stock price of a time period\n",
    "- Close: Closing stock price of a time period\n",
    "- High: Highest stock price of a time period\n",
    "- Low: Lowest stock price of a time period\n",
    "\n",
    "Note that these metrics can be analyzed for various time intervals (e.g., daily, hourly, 15 minutes, 5 minutes, etc.), but in this workshop, we'll focus on daily prices over the course of multiple years. In theory, you could build a model on any interval of time you have data for, but exploring the benefits and drawbacks of such variations are outside of the scope of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_source = 'kaggle' # alphavantage or kaggle\n",
    "\n",
    "if data_source == 'alphavantage':\n",
    "    api_key = '<API_KEY>'\n",
    "\n",
    "    # American Airlines stock market prices\n",
    "    ticker = 'AAL'\n",
    "\n",
    "    # JSON file with all the stock market data for AAL from the last 20 years\n",
    "    url_string = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&outputsize=full&apikey={api_key}'\n",
    "\n",
    "    # Save data to this file\n",
    "    file_to_save = f'../data/raw/stock_market_data-{ticker}.csv'\n",
    "\n",
    "    # If you haven't already saved data,\n",
    "    # Go ahead and grab the data from the url\n",
    "    # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "    if not os.path.exists(file_to_save):\n",
    "        with urllib.request.urlopen(url_string) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            # extract stock market data\n",
    "            data = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(columns=['Date', 'Low', 'High', 'Close', 'Open'])\n",
    "            for k,v in data.items():\n",
    "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                data_row = [date.date(), float(v['3. low']), float(v['2. high']), float(v['4. close']), float(v['1. open'])]\n",
    "                df.loc[-1,:] = data_row\n",
    "                df.index = df.index + 1\n",
    "        print(f'Data saved to : {file_to_save}')\n",
    "        df.to_csv(file_to_save)\n",
    "\n",
    "    # If the data is already there, just load it from the CSV\n",
    "    else:\n",
    "        print('File already exists. Loading data from CSV')\n",
    "        df = pd.read_csv(file_to_save)\n",
    "\n",
    "else:\n",
    "    # You will be using HP's data. Feel free to experiment with other data.\n",
    "    # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "    df = pd.read_csv(os.path.join('../data/external/Stocks', 'hpq.us.txt'), delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close'])\n",
    "    print('Loaded data from the Kaggle repository')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and Check Data\n",
    "\n",
    "Note that it is extremely important for time series data to be ordered by time, otherwise you would be training your model on some arbitrary sequence of observations which may be detrimental to its efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "plt.plot(range(df.shape[0]), (df['Low'] + df['High']) / 2.0)\n",
    "plt.xticks(range(0, df.shape[0], 500), df['Date'].loc[::500], rotation=45)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Mid Price', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "As per usual, you should split your data into training and testing sets, so your model is validated upon its predicitons for observations it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the mid prices from the highest and lowest\n",
    "high_prices = df.loc[:, 'High'].to_numpy()\n",
    "low_prices = df.loc[:, 'Low'].to_numpy()\n",
    "mid_prices = (high_prices + low_prices) / 2.0\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data = mid_prices[:11000]\n",
    "test_data = mid_prices[11000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data\n",
    "\n",
    "Before training a model, you must normalize the data. Since different time periods of data have different value ranges, we normalize the data by \"binning\" the full time series into windows of some specified size (in this case it is 2500). We then smooth **only** the training data, using the exponential moving average, to reduce the amount of noise our models encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1, 1)\n",
    "test_data = test_data.reshape(-1, 1)\n",
    "\n",
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0, 10000, smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di + smoothing_window_size, :])\n",
    "    train_data[di:di + smoothing_window_size, :] = scaler.transform(train_data[di:di + smoothing_window_size, :])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di + smoothing_window_size:, :])\n",
    "train_data[di + smoothing_window_size:, :] = scaler.transform(train_data[di + smoothing_window_size:, :])\n",
    "\n",
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)\n",
    "\n",
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "  EMA = gamma * train_data[ti] + (1 - gamma) * EMA\n",
    "  train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Step Ahead Prediction via Averaging\n",
    "\n",
    "We will compare different methods of modeling the stock price time series we have based on Mean Squared Error (MSE), which is calculated by averaging the squared error of each prediction we generate over all observations.\n",
    "\n",
    "### Standard Average\n",
    "$$x_{t+1}=\\frac{1}{N}\\sum_{i=t-N}^t x_i$$\n",
    "In this case, we're saying the prediction at time $t+1$ is the average of the stock prices observed within a window of time $t-N$ to time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size, N):\n",
    "\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred_idx, 'Date']\n",
    "\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx - window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1] - train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print(f'MSE error for standard averaging: {0.5 * np.mean(mse_errors):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(range(df.shape[0]), all_mid_data, color='b', label='True')\n",
    "plt.plot(range(window_size, N), std_avg_predictions, color='orange', label='Prediction')\n",
    "# plt.xticks(range(0, df.shape[0], 50), df['Date'].loc[::50], rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model's predictions follow the actual behavior of the stock prices fairly accurately, although it seems to lag behind the actual price movement in the market by a few days. It seems as though this model is relatively useful for making short-term price predictions (i.e., a day or two ahead), but we will continue to investigate further.\n",
    "\n",
    "### Exponential Moving Average\n",
    "$$x_{t+1}=EMA_t=\\gamma\\times EMA_{t-1}+(1-\\gamma)x_t$$\n",
    "Here, $EMA_0=0$ and $EMA$ is the exponential moving average value you maintain over time. When predicting price for time $t+1$, $\\gamma$ dictates how the immediately preceding observation (time $t$) is weighted against the prior moving average for time $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1, N):\n",
    "\n",
    "    running_mean = running_mean * decay + (1.0 - decay)*train_data[pred_idx - 1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1] - train_data[pred_idx])**2)\n",
    "    run_avg_x.append(date)\n",
    "\n",
    "print(f'MSE error for EMA averaging: {0.5 * np.mean(mse_errors):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(range(df.shape[0]), all_mid_data, color='b', label='True')\n",
    "plt.plot(range(0, N), run_avg_predictions, color='orange', label='Prediction')\n",
    "#plt.xticks(range(0, df.shape[0], 50), df['Date'].loc[::50], rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that the line of predictions nearly perfectly mirrors the actual stock price movement, but is it really that useful? In practical applications, you would ideally like to be able to make predictions for times $t+1$, $t+2$, etc. For the two models we just explored, however, you're only ever able to make a single prediction for the subsequent period of time (time $t+1$). What if you instead wanted to make a prediction 30 days in advance? For this purpose, we will explore the use of long short-term memory nueral networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}