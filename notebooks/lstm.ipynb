{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Run Set Up Code from Intro Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_source = 'kaggle'  # alphavantage or kaggle\n",
    "\n",
    "if data_source == 'alphavantage':\n",
    "    api_key = 'OW9OP1LMX4XZDQYJ'\n",
    "\n",
    "    # American Airlines stock market prices\n",
    "    ticker = 'AAL'\n",
    "\n",
    "    # JSON file with all the stock market data for AAL from the last 20 years\n",
    "    url_string = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&outputsize=full&apikey={api_key}'\n",
    "\n",
    "    # Save data to this file\n",
    "    file_to_save = f'../data/raw/stock_market_data-{ticker}.csv'\n",
    "\n",
    "    # If you haven't already saved data,\n",
    "    # Go ahead and grab the data from the url\n",
    "    # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "    if not os.path.exists(file_to_save):\n",
    "        with urllib.request.urlopen(url_string) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            # extract stock market data\n",
    "            data = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(columns=['Date', 'Low', 'High', 'Close', 'Open'])\n",
    "            for k, v in data.items():\n",
    "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                data_row = [date.date(), float(v['3. low']), float(v['2. high']), float(v['4. close']),\n",
    "                            float(v['1. open'])]\n",
    "                df.loc[-1, :] = data_row\n",
    "                df.index = df.index + 1\n",
    "        print(f'Data saved to : {file_to_save}')\n",
    "        df.to_csv(file_to_save)\n",
    "\n",
    "    # If the data is already there, just load it from the CSV\n",
    "    else:\n",
    "        print('File already exists. Loading data from CSV')\n",
    "        df = pd.read_csv(file_to_save)\n",
    "\n",
    "else:\n",
    "    # You will be using HP's data. Feel free to experiment with other data.\n",
    "    # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "    df = pd.read_csv(os.path.join('../data/external/Stocks', 'hpq.us.txt'), delimiter=',',\n",
    "                     usecols=['Date', 'Open', 'High', 'Low', 'Close'])\n",
    "    print('Loaded data from the Kaggle repository')\n",
    "\n",
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "df.head()\n",
    "\n",
    "# First calculate the mid prices from the highest and lowest\n",
    "high_prices = df.loc[:, 'High'].to_numpy()\n",
    "low_prices = df.loc[:, 'Low'].to_numpy()\n",
    "mid_prices = (high_prices + low_prices) / 2.0\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data = mid_prices[:11000]\n",
    "test_data = mid_prices[11000:]\n",
    "\n",
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1, 1)\n",
    "test_data = test_data.reshape(-1, 1)\n",
    "\n",
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0, 10000, smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di + smoothing_window_size, :])\n",
    "    train_data[di:di + smoothing_window_size, :] = scaler.transform(train_data[di:di + smoothing_window_size, :])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di + smoothing_window_size:, :])\n",
    "train_data[di + smoothing_window_size:, :] = scaler.transform(train_data[di + smoothing_window_size:, :])\n",
    "\n",
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)\n",
    "\n",
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "    EMA = gamma * train_data[ti] + (1 - gamma) * EMA\n",
    "    train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data, test_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Long Short-Term Memory Models\n",
    "\n",
    "Long Short-Term Memory models are powerful time-series models which can make predictions an arbitrary number of steps (i.e., periods of time into the future). An LSTM module, or cell, is comprised of the following components:\n",
    "- Cell state ($c_t$) - This represents the internal memory of the cell which stores both short term memory and long-term memories.\n",
    "- Hidden state ($h_t$) - This is output state information calculated with respect to current input, previous hidden state and current cell input which you eventually use to predict the future stock market prices. Additionally, the hidden state can decide to only retrive the short or long-term or both types of memory stored in the cell state to make the next prediction.\n",
    "- Input gate ($i_t$) - Decides how much information from current input flows to the cell state.\n",
    "- Forget gate ($f_t$) - Decides how much information from the current input and the previous cell state flows into the current cell state.\n",
    "- Output gate ($o_t$) - Decides how much information from the current cell state flows into the hidden state, so that if needed LSTM can only pick the long-term memories or short-term memories and long-term memories.\n",
    "\n",
    "The image below illustrates the composition of an LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<img src='img/lstm.png'>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations for calculating each of the components are as follows:\n",
    "- $i_t=\\sigma(W_{ix}x_t+W_{ih}h_{t-1}+b_i)$\n",
    "- $\\tilde{c}_t=\\sigma(W_{cx}x_t+W_{ch}h_{t-1}+b_c)$\n",
    "- $f_t=\\sigma(W_{fx}x_t+W_{fh}h_{t-1}+b_f)$\n",
    "- $c_t=f_tc_{t-1}+i_t\\tilde{c}_t$\n",
    "- $o_t=\\sigma(W_{ox}x_t+W_{oh}h_{t-1}+b_o)$\n",
    "- $h_t=o_t\\tanh{(c_t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation and Augmentation\n",
    "\n",
    "First, we need to create a data generator to train our LSTM model. The `.unroll_batches(...)` method will output a set of a specified number of batches of input data, ordered sequentially. Each batch of data will be of the specified size and will have a corresponding batch of output data.\n",
    "\n",
    "To make our model more robust, we will not make the output for $x_t$ always be $x_{t+1}$. Instead, we will randomly sample an output from the set $x_{t+1},\\ x_{t+2},\\ ...,\\ x_{t+N}$ where $N$ is a small window size. In essence, we will randomly select an output for $x_t$ which can be any observation in the time series that comes after $x_t$ and that falls within the specified window of the series, which is of size $N$. Note that, we are assuming that $x_{t+1},\\ x_{t+2},\\ ...,\\ x_{t+N}$ will be relatively close to each other in the series. The following image illustrates this data augmentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<img src='img/batch.png'>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorSeq(object):\n",
    "\n",
    "    def __init__(self, prices, batch_size, num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length // self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros(self._batch_size, dtype=np.float32)\n",
    "        batch_labels = np.zeros(self._batch_size, dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b] + 1 >= self._prices_length:\n",
    "                # self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0, (b + 1) * self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b] = self._prices[self._cursor[b] + np.random.randint(0, 5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._prices_length\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data, unroll_labels = [], []\n",
    "        init_data, init_label = None, None\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()\n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0, min((b + 1) * self._segments, self._prices_length - 1))\n",
    "\n",
    "\n",
    "dg = DataGeneratorSeq(train_data, 5, 5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui, (dat, lbl) in enumerate(zip(u_data, u_labels)):\n",
    "    print('\\n\\nUnrolled index %d' % ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ', dat)\n",
    "    print('\\n\\tOutput:', lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters\n",
    "\n",
    "Here we will define several hyperparamters which can be tweaked to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1  # Dimensionality of the data. Since your data is 1-D this would be 1\n",
    "num_unrollings = 50  # Number of time steps you look into the future.\n",
    "batch_size = 500  # Number of samples in a batch\n",
    "num_nodes = [200, 200, 150]  # Number of hidden nodes (neurons) in each layer of the deep LSTM stack we're using\n",
    "n_layers = len(num_nodes)  # number of layers\n",
    "dropout = 0.2  # dropout amount\n",
    "\n",
    "tf.compat.v1.reset_default_graph()  # This is important in case you run this multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Inputs and Outputs\n",
    "\n",
    "Now we will define placeholders for training inputs and labels. We create a list of input placeholders, where each placeholder contains a single batch of data. This list has `num_unrollings` placeholders specified which will all be used at once for a single optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs, train_outputs = [], []\n",
    "\n",
    "# You unroll the input over time defining placeholders for each time step\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size, D], name='train_inputs_%d' % ui))\n",
    "    train_outputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size, 1], name='train_outputs_%d' % ui))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Parameters of the LSTM and Regression Layer\n",
    "\n",
    "Our model has three layers of LSTMs and a linear regression layer, denoted by `w` and `b`, which uses the output of the final LSTM cell to produce a prediction for the stock price at the next time step. Dropout-implemented LSTM cells are used to improve performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cells = [\n",
    "    tf.compat.v1.nn.rnn_cell.LSTMCell(\n",
    "        num_units=num_nodes[li],\n",
    "        state_is_tuple=True,\n",
    "        initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "            scale=1.0,\n",
    "            mode=\"fan_avg\",\n",
    "            distribution=\"uniform\"\n",
    "        )\n",
    "    )\n",
    "    for li in range(n_layers)\n",
    "]\n",
    "\n",
    "drop_lstm_cells = [\n",
    "    tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
    "        lstm, input_keep_prob=1.0, output_keep_prob=1.0 - dropout, state_keep_prob=1.0 - dropout\n",
    "    )\n",
    "    for lstm in lstm_cells\n",
    "]\n",
    "drop_multi_cell = tf.keras.layers.StackedRNNCells(drop_lstm_cells)\n",
    "multi_cell = tf.keras.layers.StackedRNNCells(lstm_cells)\n",
    "\n",
    "w = tf.compat.v1.get_variable('w', shape=[num_nodes[-1], 1],\n",
    "                              initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\",\n",
    "                                                                                          distribution=\"uniform\"))\n",
    "b = tf.compat.v1.get_variable('b', initializer=tf.random.uniform([1], -0.1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating LSTM Output and Feeding it to the Regression Layer to Get Final Prediction\n",
    "\n",
    "Now, we create TensorFlow variables `c` and `h` which hold the cell state and hidden state of the LSTM cell. We then transform the training inputs and feed them to the `dynamic_rnn(...)` function which calculates LSTM outputs, and split that output back into `num_unrollings` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "c, h = [],[]\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "  c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  initial_state.append(tf.compat.v1.nn.rnn_cell.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n",
    "# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n",
    "\n",
    "# all_outputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.compat.v1.nn.dynamic_rnn(\n",
    "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n",
    "    time_major = True, dtype=tf.float32)\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
    "\n",
    "all_outputs = tf.compat.v1.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "\n",
    "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation and Optimizer\n",
    "\n",
    "Now, we calculate the loss of our predictions by summing together the Mean Squared Error (MSE) of each prediction within each batch of data. Then, we define an optimizer which will be used to optimize the nueral network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.compat.v1.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
    "                             [tf.compat.v1.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "  for ui in range(num_unrollings):\n",
    "    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.compat.v1.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.compat.v1.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.compat.v1.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.compat.v1.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)\n",
    "\n",
    "# Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Related Calculations\n",
    "\n",
    "The last step before running the LSTM is to define prediction-related TensorFlow operations. We first define a placeholder for feeding in our inputs (`sample_inputs`), and define state variables for prediction (`sample_c` and `sample_h`). Finally, we calculate predictions with the `dynamic_rnn(...)` function and send the output through the regression layer (`w` and `b`). We also define the `reset_sample_state` operation which resets the cell state and hidden state of the LSTM cells, and should be executed just before making each batch of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Defining prediction related TF functions')\n",
    "\n",
    "sample_inputs = tf.compat.v1.placeholder(tf.float32, shape=[1,D])\n",
    "\n",
    "# Maintaining LSTM state for prediction stage\n",
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "for li in range(n_layers):\n",
    "  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  initial_sample_state.append(tf.compat.v1.nn.rnn_cell.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.compat.v1.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
    "                               *[tf.compat.v1.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "sample_outputs, sample_state = tf.compat.v1.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
    "                                   initial_state=tuple(initial_sample_state),\n",
    "                                   time_major = True,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.compat.v1.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
    "                              [tf.compat.v1.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
    "  sample_prediction = tf.compat.v1.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the LSTM\n",
    "\n",
    "Here we train and predict stock price movements for several (30) epochs and see how the network performs over time. The following procedure is used:\n",
    "1. Define a test set of starting points (`test_points_seq`) on the time series to evaluate the model on\n",
    "2. For each epoch\n",
    "    1. For full sequence length of training data\n",
    "        1. Unroll a set of `num_unrollings` batches\n",
    "        2. Train the neural network with the unrolled batches\n",
    "    2. Calculate the average training loss\n",
    "    3. For each starting point in the test set\n",
    "        1. Update the LSTM state by iterating through the previous `num_unrollings` data points found before the test point\n",
    "        2. Make predictions for `n_predict_once` steps continuously, using the previous prediction as the current input\n",
    "        3. Calculate the MSE loss between the `n_predict_once` points predicted and the true stock prices at those time stamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "valid_summary = 1 # Interval you make test predictions\n",
    "\n",
    "n_predict_once = 50 # Number of steps you continously predict for\n",
    "\n",
    "train_seq_length = train_data.size # Full length of the training data\n",
    "\n",
    "train_mse_ot = [] # Accumulate Train losses\n",
    "test_mse_ot = [] # Accumulate Test loss\n",
    "predictions_over_time = [] # Accumulate predictions\n",
    "\n",
    "session = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "# Used for decaying learning rate\n",
    "loss_nondecrease_count = 0\n",
    "loss_nondecrease_threshold = 2 # If the test error hasn't increased in this many steps, decrease learning rate\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# Define data generator\n",
    "data_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings)\n",
    "\n",
    "x_axis_seq = []\n",
    "\n",
    "# Points you start your test predictions from\n",
    "test_points_seq = np.arange(11000,12000,50).tolist()\n",
    "\n",
    "for ep in range(epochs):       \n",
    "\n",
    "    # ========================= Training =====================================\n",
    "    for step in range(train_seq_length//batch_size):\n",
    "\n",
    "        u_data, u_labels = data_gen.unroll_batches()\n",
    "\n",
    "        feed_dict = {}\n",
    "        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n",
    "            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n",
    "\n",
    "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n",
    "\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        average_loss += l\n",
    "\n",
    "    # ============================ Validation ==============================\n",
    "    if (ep+1) % valid_summary == 0:\n",
    "\n",
    "      average_loss = average_loss/(valid_summary*(train_seq_length//batch_size))\n",
    "\n",
    "      # The average loss\n",
    "      if (ep+1)%valid_summary==0:\n",
    "        print('Average loss at step %d: %f' % (ep+1, average_loss))\n",
    "\n",
    "      train_mse_ot.append(average_loss)\n",
    "\n",
    "      average_loss = 0 # reset loss\n",
    "\n",
    "      predictions_seq = []\n",
    "\n",
    "      mse_test_loss_seq = []\n",
    "\n",
    "      # ===================== Updating State and Making Predicitons ========================\n",
    "      for w_i in test_points_seq:\n",
    "        mse_test_loss = 0.0\n",
    "        our_predictions = []\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          # Only calculate x_axis values in the first validation epoch\n",
    "          x_axis=[]\n",
    "\n",
    "        # Feed in the recent past behavior of stock prices\n",
    "        # to make predictions from that point onwards\n",
    "        for tr_i in range(w_i-num_unrollings+1,w_i-1):\n",
    "          current_price = all_mid_data[tr_i]\n",
    "          feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n",
    "          _ = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "        feed_dict = {}\n",
    "\n",
    "        current_price = all_mid_data[w_i-1]\n",
    "\n",
    "        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
    "\n",
    "        # Make predictions for this many steps\n",
    "        # Each prediction uses previous prediciton as it's current input\n",
    "        for pred_i in range(n_predict_once):\n",
    "\n",
    "          pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "          our_predictions.append(np.asscalar(pred))\n",
    "\n",
    "          feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
    "\n",
    "          if (ep+1)-valid_summary==0:\n",
    "            # Only calculate x_axis values in the first validation epoch\n",
    "            x_axis.append(w_i+pred_i)\n",
    "\n",
    "          mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n",
    "\n",
    "        session.run(reset_sample_states)\n",
    "\n",
    "        predictions_seq.append(np.array(our_predictions))\n",
    "\n",
    "        mse_test_loss /= n_predict_once\n",
    "        mse_test_loss_seq.append(mse_test_loss)\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          x_axis_seq.append(x_axis)\n",
    "\n",
    "      current_test_mse = np.mean(mse_test_loss_seq)\n",
    "\n",
    "      # Learning rate decay logic\n",
    "      if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
    "          loss_nondecrease_count += 1\n",
    "      else:\n",
    "          loss_nondecrease_count = 0\n",
    "\n",
    "      if loss_nondecrease_count > loss_nondecrease_threshold :\n",
    "            session.run(inc_gstep)\n",
    "            loss_nondecrease_count = 0\n",
    "            print('\\tDecreasing learning rate by 0.5')\n",
    "\n",
    "      test_mse_ot.append(current_test_mse)\n",
    "      print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
    "      predictions_over_time.append(predictions_seq)\n",
    "      print('\\tFinished Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Predictions\n",
    "\n",
    "Generally speaking, the MSE of our predictions goes down as the model continues to be trained. We can compare the best MSE of our model above back to that of our standard moving average model, at which point we see that our LSTM neural network performed significantly better. Make sure to replace the value of `best_prediction_epoch` with the step number of the epoch in which the test MSE is lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction_epoch = 18 # replace this with the epoch that you got the best results when running the plotting code\n",
    "\n",
    "plt.figure(figsize = (18,18))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "\n",
    "# Plotting how the predictions change over time\n",
    "# Plot older predictions with low alpha and newer predictions with high alpha\n",
    "start_alpha = 0.25\n",
    "alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))\n",
    "for p_i,p in enumerate(predictions_over_time[::3]):\n",
    "    for xval,yval in zip(x_axis_seq,p):\n",
    "        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n",
    "\n",
    "plt.title('Evolution of Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12075)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "# Predicting the best test prediction you got\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch - 1]):\n",
    "    plt.plot(xval,yval,color='r')\n",
    "\n",
    "plt.title('Best Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12075)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model is making predictions that fall roughly in the range of 0 to 1.0 (i.e., not the actual stock prices). This is because we're trying to predict the movement of the stock prices rather than the prices themselves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}